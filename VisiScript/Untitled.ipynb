{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9af5b8-2823-4807-9d13-fcb4f8b3df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d17354-5393-4859-9c4b-f95f86123644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042671e-164f-4051-ad4c-d4184464aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b09dbe-a2e4-492b-b564-9fb93c840995",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c huggingface transformers -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b1de7-165b-4508-960e-567de86184c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4a8d8-b9be-470c-a9c7-381dbf822c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)  # Should print the installed version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bafbad2-dfaa-431e-813f-5055a40f1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f724bcdb-ad69-479a-a3ea-e5239952b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration, SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeda771-d5a1-456f-bf28-191861c1f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b10c19-e3e7-478b-941d-7dd9056e479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pytorch torchvision torchaudio -c pytorch -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2094f-e015-49cf-a0cf-8ad9ee081294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Should print the installed PyTorch version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21908cef-de96-4bb3-a7b3-044930a0fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84850436-08c2-40d8-94d4-440ec71c5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83e706-099c-4fae-9a73-e3ee041db90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533909d7-97fb-41aa-9221-325bb1a952af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b46bfb-5c8a-4d13-8e32-e3918e0998f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge soundfile -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14865f4-bf39-44a0-bd70-02bdc5bfe053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fbae4-9ea6-4b6e-b63e-7a5f0360bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90662ebc-ad33-4be2-abf6-09989f020205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "import os\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "# Model Description\n",
    "model_description = \"\"\"\n",
    "This application utilizes image captioning and text-to-speech models to generate a caption for an uploaded image \n",
    "and convert the caption into speech.\n",
    "\n",
    "The image captioning model is based on [Salesforce's BLIP architecture](https://huggingface.co/Salesforce/blip-image-captioning-base), which can generate descriptive captions for images.\n",
    "\n",
    "The text-to-speech model, based on [Microsoft's SpeechT5](https://huggingface.co/microsoft/speecht5_tts), converts the generated caption into speech with the help of a \n",
    "HiFiGAN vocoder.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@st.cache_resource\n",
    "def initialize_image_captioning():\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    return processor, model\n",
    "\n",
    "@st.cache_resource\n",
    "def initialize_speech_synthesis():\n",
    "    processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "    model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "    vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "    embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "    speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "    return processor, model, vocoder, speaker_embeddings\n",
    "\n",
    "def generate_caption(processor, model, image):\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs)\n",
    "    output_caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return output_caption\n",
    "\n",
    "def generate_speech(processor, model, vocoder, speaker_embeddings, caption):\n",
    "    inputs = processor(text=caption, return_tensors=\"pt\")\n",
    "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "    sf.write(\"speech.wav\", speech.numpy(), samplerate=16000)\n",
    "\n",
    "def play_sound():\n",
    "    audio_file = open(\"speech.wav\", 'rb')\n",
    "    audio_bytes = audio_file.read()\n",
    "    st.audio(audio_bytes, format='audio/wav')\n",
    "\n",
    "def visualize_speech():\n",
    "    data, samplerate = sf.read(\"speech.wav\")\n",
    "    duration = len(data) / samplerate\n",
    "\n",
    "    # Create time axis\n",
    "    time = np.linspace(0., duration, len(data))\n",
    "\n",
    "     # Plot the speech waveform\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.plot(time, data)\n",
    "    ax.set(xlabel=\"Time (s)\", ylabel=\"Amplitude\", title=\"Speech Waveform\")\n",
    "\n",
    "    # Display the plot using st.pyplot()\n",
    "    st.pyplot(fig)\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(\n",
    "    page_title=\"Image-to-Speech\",\n",
    "    page_icon=\"ðŸ“¸\",\n",
    "    initial_sidebar_state=\"collapsed\",\n",
    "    menu_items={\n",
    "        'Get Help': 'https://www.extremelycoolapp.com/help',\n",
    "        'Report a bug': \"https://www.extremelycoolapp.com/bug\",\n",
    "        'About': \"# This is a header. This is an *extremely* cool app!\"\n",
    "    }\n",
    ")\n",
    "\n",
    "    st.sidebar.markdown(\"---\")\n",
    "    st.sidebar.markdown(\"Developed by Alim Tleuliyev\")\n",
    "    st.sidebar.markdown(\"Contact: [alim.tleuliyev@nu.edu.kz](mailto:alim.tleuliyev@nu.edu.kz)\")\n",
    "    st.sidebar.markdown(\"GitHub: [Repo](https://github.com/AlimTleuliyev/image-to-audio)\")\n",
    "\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <style>\n",
    "        .container {\n",
    "            max-width: 800px;\n",
    "        }\n",
    "        .title {\n",
    "            text-align: center;\n",
    "            font-size: 32px;\n",
    "            font-weight: bold;\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        .description {\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .instructions {\n",
    "            margin-bottom: 20px;\n",
    "            padding: 10px;\n",
    "            background-color: #f5f5f5;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    st.markdown(\"<div class='title'>Image Captioning and Text-to-Speech</div>\", unsafe_allow_html=True)\n",
    "    col1, col2, col3 = st.columns([1,2,1])\n",
    "\n",
    "    with col1:\n",
    "        st.write(\"\")\n",
    "\n",
    "    with col2:\n",
    "        st.image(\"images/logo.png\", use_column_width=True, caption=\"Generated by DALL-E\")\n",
    "\n",
    "    with col3:\n",
    "        st.write(\"\")\n",
    "\n",
    "    # Model Description\n",
    "    st.markdown(\"<div class='description'>\" + model_description + \"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "    # Instructions\n",
    "    with st.expander(\"Instructions\"):\n",
    "        st.markdown(\"1. Upload an image or provide the URL of an image.\")\n",
    "        st.markdown(\"2. Click the 'Generate Caption and Speech' button.\")\n",
    "        st.markdown(\"3. The generated caption will be displayed, and the speech will start playing.\")\n",
    "\n",
    "\n",
    "    # Choose image source\n",
    "    image_source = st.radio(\"Select Image Source:\", (\"Upload Image\", \"Open from URL\"))\n",
    "\n",
    "    image = None\n",
    "\n",
    "    if image_source == \"Upload Image\":\n",
    "        # File uploader for image\n",
    "        uploaded_file = st.file_uploader(\"Upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
    "        if uploaded_file is not None:\n",
    "            image = Image.open(uploaded_file)\n",
    "        else:\n",
    "            image = None\n",
    "\n",
    "    else:\n",
    "        # Input box for image URL\n",
    "        url = st.text_input(\"Enter the image URL:\")\n",
    "        if url:\n",
    "            try:\n",
    "                response = requests.get(url, stream=True)\n",
    "                if response.status_code == 200:\n",
    "                    image = Image.open(response.raw)\n",
    "                else:\n",
    "                    st.error(\"Error loading image from URL.\")\n",
    "                    image = None\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                st.error(f\"Error loading image from URL: {e}\")\n",
    "                image = None\n",
    "\n",
    "    # Generate caption and play sound button\n",
    "    if image is not None:\n",
    "        # Display the uploaded image\n",
    "        st.image(image, caption='Uploaded Image', use_column_width=True)\n",
    "\n",
    "        # Initialize image captioning models\n",
    "        caption_processor, caption_model = initialize_image_captioning()\n",
    "\n",
    "        # Initialize speech synthesis models\n",
    "        speech_processor, speech_model, speech_vocoder, speaker_embeddings = initialize_speech_synthesis()\n",
    "\n",
    "        # Generate caption\n",
    "        with st.spinner(\"Generating Caption...\"):\n",
    "            output_caption = generate_caption(caption_processor, caption_model, image)\n",
    "\n",
    "        # Display the caption\n",
    "        st.subheader(\"Caption:\")\n",
    "        st.write(output_caption)\n",
    "        \n",
    "        # Generate speech from the caption\n",
    "        with st.spinner(\"Generating Speech...\"):\n",
    "            generate_speech(speech_processor, speech_model, speech_vocoder, speaker_embeddings, output_caption)\n",
    "\n",
    "        \n",
    "        st.subheader(\"Audio:\")\n",
    "        # Play the generated sound\n",
    "        play_sound()\n",
    "\n",
    "        # Visualize the speech waveform\n",
    "        with st.expander(\"See visualization\"):\n",
    "            visualize_speech()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ab49b-4d50-4e58-a8d1-c2dea5b66dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469bbe96-fc1c-4e7a-b177-7cc76d100322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "import os\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "# Model Description\n",
    "model_description = \"\"\"\n",
    "This application utilizes image captioning and text-to-speech models to generate a caption for an uploaded image \n",
    "and convert the caption into speech.\n",
    "\n",
    "The image captioning model is based on [Salesforce's BLIP architecture](https://huggingface.co/Salesforce/blip-image-captioning-base), which can generate descriptive captions for images.\n",
    "\n",
    "The text-to-speech model, based on [Microsoft's SpeechT5](https://huggingface.co/microsoft/speecht5_tts), converts the generated caption into speech with the help of a \n",
    "HiFiGAN vocoder.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@st.cache_resource\n",
    "def initialize_image_captioning():\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    return processor, model\n",
    "\n",
    "@st.cache_resource\n",
    "def initialize_speech_synthesis():\n",
    "    processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "    model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "    vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "    embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "    speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "    return processor, model, vocoder, speaker_embeddings\n",
    "\n",
    "def generate_caption(processor, model, image):\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs)\n",
    "    output_caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return output_caption\n",
    "\n",
    "def generate_speech(processor, model, vocoder, speaker_embeddings, caption):\n",
    "    inputs = processor(text=caption, return_tensors=\"pt\")\n",
    "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "    sf.write(\"speech.wav\", speech.numpy(), samplerate=16000)\n",
    "\n",
    "def play_sound():\n",
    "    audio_file = open(\"speech.wav\", 'rb')\n",
    "    audio_bytes = audio_file.read()\n",
    "    st.audio(audio_bytes, format='audio/wav')\n",
    "\n",
    "def visualize_speech():\n",
    "    data, samplerate = sf.read(\"speech.wav\")\n",
    "    duration = len(data) / samplerate\n",
    "\n",
    "    # Create time axis\n",
    "    time = np.linspace(0., duration, len(data))\n",
    "\n",
    "     # Plot the speech waveform\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.plot(time, data)\n",
    "    ax.set(xlabel=\"Time (s)\", ylabel=\"Amplitude\", title=\"Speech Waveform\")\n",
    "\n",
    "    # Display the plot using st.pyplot()\n",
    "    st.pyplot(fig)\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(\n",
    "    page_title=\"Image-to-Speech\",\n",
    "    page_icon=\"ðŸ“¸\",\n",
    "    initial_sidebar_state=\"collapsed\",\n",
    "    menu_items={\n",
    "        'Get Help': 'https://www.extremelycoolapp.com/help',\n",
    "        'Report a bug': \"https://www.extremelycoolapp.com/bug\",\n",
    "        'About': \"# This is a header. This is an *extremely* cool app!\"\n",
    "    }\n",
    ")\n",
    "\n",
    "    st.sidebar.markdown(\"---\")\n",
    "    st.sidebar.markdown(\"Developed by Alim Tleuliyev\")\n",
    "    st.sidebar.markdown(\"Contact: [alim.tleuliyev@nu.edu.kz](mailto:alim.tleuliyev@nu.edu.kz)\")\n",
    "    st.sidebar.markdown(\"GitHub: [Repo](https://github.com/AlimTleuliyev/image-to-audio)\")\n",
    "\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <style>\n",
    "        .container {\n",
    "            max-width: 800px;\n",
    "        }\n",
    "        .title {\n",
    "            text-align: center;\n",
    "            font-size: 32px;\n",
    "            font-weight: bold;\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        .description {\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .instructions {\n",
    "            margin-bottom: 20px;\n",
    "            padding: 10px;\n",
    "            background-color: #f5f5f5;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    st.markdown(\"<div class='title'>Image Captioning and Text-to-Speech</div>\", unsafe_allow_html=True)\n",
    "    col1, col2, col3 = st.columns([1,2,1])\n",
    "\n",
    "    with col1:\n",
    "        st.write(\"\")\n",
    "\n",
    "    with col2:\n",
    "        st.image(\"images/logo.png\", use_column_width=True, caption=\"Generated by DALL-E\")\n",
    "\n",
    "    with col3:\n",
    "        st.write(\"\")\n",
    "\n",
    "    # Model Description\n",
    "    st.markdown(\"<div class='description'>\" + model_description + \"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "    # Instructions\n",
    "    with st.expander(\"Instructions\"):\n",
    "        st.markdown(\"1. Upload an image or provide the URL of an image.\")\n",
    "        st.markdown(\"2. Click the 'Generate Caption and Speech' button.\")\n",
    "        st.markdown(\"3. The generated caption will be displayed, and the speech will start playing.\")\n",
    "\n",
    "\n",
    "    # Choose image source\n",
    "    image_source = st.radio(\"Select Image Source:\", (\"Upload Image\", \"Open from URL\"))\n",
    "\n",
    "    image = None\n",
    "\n",
    "    if image_source == \"Upload Image\":\n",
    "        # File uploader for image\n",
    "        uploaded_file = st.file_uploader(\"Upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
    "        if uploaded_file is not None:\n",
    "            image = Image.open(uploaded_file)\n",
    "        else:\n",
    "            image = None\n",
    "\n",
    "    else:\n",
    "        # Input box for image URL\n",
    "        url = st.text_input(\"Enter the image URL:\")\n",
    "        if url:\n",
    "            try:\n",
    "                response = requests.get(url, stream=True)\n",
    "                if response.status_code == 200:\n",
    "                    image = Image.open(response.raw)\n",
    "                else:\n",
    "                    st.error(\"Error loading image from URL.\")\n",
    "                    image = None\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                st.error(f\"Error loading image from URL: {e}\")\n",
    "                image = None\n",
    "\n",
    "    # Generate caption and play sound button\n",
    "    if image is not None:\n",
    "        # Display the uploaded image\n",
    "        st.image(image, caption='Uploaded Image', use_column_width=True)\n",
    "\n",
    "        # Initialize image captioning models\n",
    "        caption_processor, caption_model = initialize_image_captioning()\n",
    "\n",
    "        # Initialize speech synthesis models\n",
    "        speech_processor, speech_model, speech_vocoder, speaker_embeddings = initialize_speech_synthesis()\n",
    "\n",
    "        # Generate caption\n",
    "        with st.spinner(\"Generating Caption...\"):\n",
    "            output_caption = generate_caption(caption_processor, caption_model, image)\n",
    "\n",
    "        # Display the caption\n",
    "        st.subheader(\"Caption:\")\n",
    "        st.write(output_caption)\n",
    "        \n",
    "        # Generate speech from the caption\n",
    "        with st.spinner(\"Generating Speech...\"):\n",
    "            generate_speech(speech_processor, speech_model, speech_vocoder, speaker_embeddings, output_caption)\n",
    "\n",
    "        \n",
    "        st.subheader(\"Audio:\")\n",
    "        # Play the generated sound\n",
    "        play_sound()\n",
    "\n",
    "        # Visualize the speech waveform\n",
    "        with st.expander(\"See visualization\"):\n",
    "            visualize_speech()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11d069-cd23-4001-90a8-792ad1c7db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit run C:\\Users\\khush\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc43d5-1496-4283-9ed7-1c4101b1963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run your_script.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8dd1d-8f77-4199-8343-3c5444b0e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this as app.py\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Hello, Streamlit!\")\n",
    "st.write(\"This is a simple Streamlit app.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167c0a9-0e15-4840-898c-7f105415f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da344773-ae16-48b6-ba6f-524c3a39d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Hello, Streamlit!\")\n",
    "st.write(\"This is a simple Streamlit app running from Jupyter.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c20989-05b8-4c78-8627-b8ee4520bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f16ae-e9f8-4cdf-bdca-4eb2f6ce290c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
